{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "_EwqZbj9hTSF",
        "ZuGVjTMHIukk",
        "0mkAikjuI1As"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle data download"
      ],
      "metadata": {
        "id": "_EwqZbj9hTSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "QKt2CXe7Ia_1",
        "outputId": "078a6a07-3062-41e9-aa9a-b91cb1c21b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0246ebb5-21bb-4d41-97bd-53da0dd327c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0246ebb5-21bb-4d41-97bd-53da0dd327c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"jeonhyotaek\",\"key\":\"d2ddaf4c586e0bf63051e1a4c6a74dd4\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls -1ha kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mseeQKcGIip0",
        "outputId": "d6d2f96f-27a0-48b0-f082-70669805d5f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "cHSgRdsPIkwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c copy-of-6th-goorm-project-1-text-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmM32WfAInIU",
        "outputId": "0f336b12-8a5e-4e02-cd34-0d2b18caeaf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading copy-of-6th-goorm-project-1-text-classification.zip to /content\n",
            "  0% 0.00/6.12M [00:00<?, ?B/s]\n",
            "100% 6.12M/6.12M [00:00<00:00, 94.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip copy-of-6th-goorm-project-1-text-classification.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhx8p0AYIpq8",
        "outputId": "59d4e9bf-c686-4c35-baf9-c22d4e5e889e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  copy-of-6th-goorm-project-1-text-classification.zip\n",
            "  inflating: sentiment.dev.0         \n",
            "  inflating: sentiment.dev.1         \n",
            "  inflating: sentiment.train.0       \n",
            "  inflating: sentiment.train.1       \n",
            "  inflating: test_no_label.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import requirements"
      ],
      "metadata": {
        "id": "ZuGVjTMHIukk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6r3IUmWIuZk",
        "outputId": "05233066-4335-4a41-ce04-2f7af90cf722"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 83.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    AutoConfig,\n",
        "    AdamW\n",
        ")"
      ],
      "metadata": {
        "id": "oD--G7VsIx88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "0mkAikjuI1As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id_file(task, tokenizer):\n",
        "    def make_data_strings(file_name):\n",
        "        data_strings = []\n",
        "        with open(os.path.join(file_name), 'r', encoding='utf-8') as f:\n",
        "            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n",
        "        for item in id_file_data:\n",
        "            data_strings.append(' '.join([str(k) for k in item]))\n",
        "        return data_strings\n",
        "    \n",
        "    print('it will take some times...')\n",
        "    train_pos = make_data_strings('sentiment.train.1')\n",
        "    train_neg = make_data_strings('sentiment.train.0')\n",
        "    dev_pos = make_data_strings('sentiment.dev.1')\n",
        "    dev_neg = make_data_strings('sentiment.dev.0')\n",
        "\n",
        "    print('make id file finished!')\n",
        "    return train_pos, train_neg, dev_pos, dev_neg"
      ],
      "metadata": {
        "id": "PdBzqs7vI5at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "RaCm2RRoJKyH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi84H2VyJLzl",
        "outputId": "6e3d186f-1114-4d6c-f668-06520ee4d83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copy-of-6th-goorm-project-1-text-classification.zip  sentiment.dev.1\n",
            "kaggle.json\t\t\t\t\t     sentiment.train.0\n",
            "sample_data\t\t\t\t\t     sentiment.train.1\n",
            "sentiment.dev.0\t\t\t\t\t     test_no_label.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBTCSDd_JtVF",
        "outputId": "19578593-1bae-4d42-ccee-a04344ff0ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it will take some times...\n",
            "make id file finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai9SBEfWJuxl",
        "outputId": "fea1836f-39c9-4556-9a1c-25556e39c903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['101 6581 2833 1012 102',\n",
              " '101 21688 8013 2326 1012 102',\n",
              " '101 2027 2036 2031 3679 19247 1998 3256 6949 2029 2003 2428 2204 1012 102',\n",
              " '101 2009 1005 1055 1037 2204 15174 2098 7570 22974 2063 1012 102',\n",
              " '101 1996 3095 2003 5379 1012 102',\n",
              " '101 2204 3347 2833 1012 102',\n",
              " '101 2204 2326 1012 102',\n",
              " '101 11350 1997 2154 2003 25628 1998 7167 1997 19247 1012 102',\n",
              " '101 2307 2173 2005 6265 2030 3347 27962 1998 5404 1012 102',\n",
              " '101 1996 2047 2846 3504 6429 1012 102']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(object):\n",
        "    def __init__(self, tokenizer, pos, neg):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for pos_sent in pos:\n",
        "            self.data += [self._cast_to_int(pos_sent.strip().split())]\n",
        "            self.label += [[1]]\n",
        "        for neg_sent in neg:\n",
        "            self.data += [self._cast_to_int(neg_sent.strip().split())]\n",
        "            self.label += [[0]]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample), np.array(self.label[index])"
      ],
      "metadata": {
        "id": "oEOkDR26KDWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n",
        "dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)"
      ],
      "metadata": {
        "id": "O0aUSNntKD5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(train_dataset):\n",
        "    print(item)\n",
        "    if i == 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81K9iw1jKEy0",
        "outputId": "c8bf78d8-b55b-4d19-8167-dc6f67fb884c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 101, 6581, 2833, 1012,  102]), array([1]))\n",
            "(array([  101, 21688,  8013,  2326,  1012,   102]), array([1]))\n",
            "(array([  101,  2027,  2036,  2031,  3679, 19247,  1998,  3256,  6949,\n",
            "        2029,  2003,  2428,  2204,  1012,   102]), array([1]))\n",
            "(array([  101,  2009,  1005,  1055,  1037,  2204, 15174,  2098,  7570,\n",
            "       22974,  2063,  1012,   102]), array([1]))\n",
            "(array([ 101, 1996, 3095, 2003, 5379, 1012,  102]), array([1]))\n",
            "(array([ 101, 2204, 3347, 2833, 1012,  102]), array([1]))\n",
            "(array([ 101, 2204, 2326, 1012,  102]), array([1]))\n",
            "(array([  101, 11350,  1997,  2154,  2003, 25628,  1998,  7167,  1997,\n",
            "       19247,  1012,   102]), array([1]))\n",
            "(array([  101,  2307,  2173,  2005,  6265,  2030,  3347, 27962,  1998,\n",
            "        5404,  1012,   102]), array([1]))\n",
            "(array([ 101, 1996, 2047, 2846, 3504, 6429, 1012,  102]), array([1]))\n",
            "(array([ 101, 2023, 2173, 2001, 2200, 2204, 1012,  102]), array([1]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_style(samples):\n",
        "    input_ids, labels = zip(*samples)\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "\n",
        "    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
        "\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids, labels"
      ],
      "metadata": {
        "id": "8Czctb9VKFsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "PUGZk-bojXps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHEl30Pnjcm8",
        "outputId": "68cd3b28-2654-4c1e-920f-f65eaa10d8c1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "      'name' : 'goorm_project1_test.ipynb',\n",
        "      'method' : 'grid',\n",
        "      'metric':{\n",
        "          'name': 'validation_acc',\n",
        "          'goal': 'maximize'  \n",
        "      },\n",
        "      'parameters' : {\n",
        "          'learning_rate' : {\n",
        "              'values' : [5e-5]\n",
        "          },   \n",
        "          'batch_size' :{\n",
        "              'values' : [512]\n",
        "          },\n",
        "          'epochs' : {\n",
        "              'values' : [1] \n",
        "          }\n",
        "      }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beXG6VxRjgDj",
        "outputId": "ea7c24f0-c1ad-44dd-8a42-f2b7e63e96f6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: cteknfiz\n",
            "Sweep URL: https://wandb.ai/goorm_project/uncategorized/sweeps/cteknfiz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model def"
      ],
      "metadata": {
        "id": "aaHYHOlBhjol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sweep_optimizer(input_model, optimizer, learning_rate) :\n",
        "    if optimizer == \"AdamW\" :\n",
        "        optimizer = torch.optim.AdamW(input_model.parameters(), lr = learning_rate, eps = 1e-6, weight_decay = 0.02)\n",
        "    elif optimizer == 'RMSprop' :\n",
        "        optimizer = torch.optim.RMSprop(input_model.parameters(), lr = learning_rate, weight_decay = 0.02)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "clpfqU44YFp7"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    wandb.init(project = 'test_sweep', reinit = True)\n",
        "\n",
        "    ## batch_size - sweep\n",
        "    train_batch_size = int(wandb.config.batch_size / 2)\n",
        "    eval_batch_size = wandb.config.batch_size\n",
        "\n",
        "    # dataload\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                              batch_size=train_batch_size,\n",
        "                                              shuffle=True, collate_fn=collate_fn_style,\n",
        "                                              pin_memory=True, num_workers=2)\n",
        "    dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n",
        "                                            shuffle=False, collate_fn=collate_fn_style,\n",
        "                                            num_workers=2)\n",
        "    ## random seed\n",
        "    random_seed=42\n",
        "    np.random.seed(random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # model-pretrain\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    model.to(device)\n",
        "\n",
        "    # model train - sweep\n",
        "    model.train()\n",
        "    learning_rate = wandb.config.learning_rate\n",
        "    optimizer =  AdamW(model.parameters(), lr=learning_rate, eps = 1e-6, weight_decay = 0.02)\n",
        "\n",
        "    def compute_acc(predictions, target_labels):\n",
        "        return (np.array(predictions) == np.array(target_labels)).mean()\n",
        "\n",
        "    # train_epoch\n",
        "    train_epoch = wandb.config.epochs\n",
        "    # validation_acc\n",
        "    wan_valid_acc = []\n",
        "\n",
        "    lowest_valid_loss = 9999.\n",
        "\n",
        "    #train\n",
        "    for epoch in range(train_epoch):\n",
        "        #total train loss\n",
        "        wan_train_loss = []\n",
        "\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            \n",
        "            for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                token_type_ids = token_type_ids.to(device)\n",
        "                position_ids = position_ids.to(device)\n",
        "                labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                output = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              token_type_ids=token_type_ids,\n",
        "                              position_ids=position_ids,\n",
        "                              labels=labels)\n",
        "\n",
        "                loss = output.loss\n",
        "                loss.backward()\n",
        "\n",
        "                # log the train loss / total loss\n",
        "                wandb.log({'train_batch_loss':loss.item()})\n",
        "                wan_train_loss.append(loss.item())\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "                #sum_acc\n",
        "                sum_acc = [] \n",
        "\n",
        "                if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:\n",
        "                    # Evaluate the model five times per epoch\n",
        "                    with torch.no_grad():\n",
        "                        model.eval()\n",
        "                        valid_losses = []\n",
        "                        predictions = []\n",
        "                        target_labels = []\n",
        "                        for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n",
        "                                                                                                    desc='Eval',\n",
        "                                                                                                    position=1,\n",
        "                                                                                                    leave=None):\n",
        "                            input_ids = input_ids.to(device)\n",
        "                            attention_mask = attention_mask.to(device)\n",
        "                            token_type_ids = token_type_ids.to(device)\n",
        "                            position_ids = position_ids.to(device)\n",
        "                            labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "                            output = model(input_ids=input_ids,\n",
        "                                          attention_mask=attention_mask,\n",
        "                                          token_type_ids=token_type_ids,\n",
        "                                          position_ids=position_ids,\n",
        "                                          labels=labels)\n",
        "\n",
        "                            logits = output.logits\n",
        "                            loss = output.loss\n",
        "                            valid_losses.append(loss.item())\n",
        "\n",
        "                            batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n",
        "                            batch_labels = [int(example) for example in labels]\n",
        "\n",
        "                            predictions += batch_predictions\n",
        "                            target_labels += batch_labels\n",
        "\n",
        "\n",
        "                    acc = compute_acc(predictions, target_labels)\n",
        "                    valid_loss = sum(valid_losses) / len(valid_losses)\n",
        "\n",
        "                    wan_valid_acc.append(float(acc))\n",
        "                    wandb.log({'validation_acc': sum(wan_valid_acc)/len(wan_valid_acc)})\n",
        "\n",
        "                    sum_acc.append(float(acc)) ############\n",
        "                    wandb.log({\"sum_acc\": sum(sum_acc) / len(sum_acc)}) #####\n",
        "\n",
        "\n",
        "                    if lowest_valid_loss > valid_loss:\n",
        "                        print('Acc for model which have lower valid loss: ', acc)\n",
        "                        torch.save(model.state_dict(), \"./pytorch_model.bin\")\n",
        "                        lowest_valid_loss = valid_loss\n",
        "\n",
        "                else:\n",
        "                    #avg train loss\n",
        "                    wandb.log({'avg_train_loss': sum(wan_train_loss) / len(wan_train_loss)})\n"
      ],
      "metadata": {
        "id": "CS-P9ROWh0y0"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent( sweep_id , function=train, count=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "id": "Btn0mteQjnBr",
        "outputId": "eb3d33eb-74ce-46fb-d0c4-3c5dffd422f9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6akwvf2w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221228_085900-6akwvf2w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/goorm_project/uncategorized/runs/6akwvf2w\" target=\"_blank\">young-sweep-1</a></strong> to <a href=\"https://wandb.ai/goorm_project/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/goorm_project/uncategorized/sweeps/cteknfiz\" target=\"_blank\">https://wandb.ai/goorm_project/uncategorized/sweeps/cteknfiz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 0:  20%|█▉        | 346/1732 [01:20<05:14,  4.41batch/s, loss=0.0579]\n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:01,  3.63it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.83it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.41it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.87it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.67it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.76it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.70it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.13it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  27%|██▋       | 462/1732 [01:50<04:52,  4.35batch/s, loss=0.0661]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "aGxv7iySihd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}